import tensorflow as tf
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras import Sequential, callbacks, layers, optimizers, datasets
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt

(train_input, train_target), (test_input, test_target) = datasets.imdb.load_data(num_words = 500)
train_input, val_input, train_target, val_target = train_test_split(
    train_input, train_target, test_size=0.2, random_state=42
)

train_seq = pad_sequences(train_input, maxlen=100)
val_seq = pad_sequences(val_input, maxlen=100)
test_seq = pad_sequences(test_input, maxlen=100)

model = Sequential(
    [layers.Embedding(500, 16, input_length=100),
     layers.LSTM(8, dropout=0.3, return_sequences=True),
     layers.LSTM(8, dropout=0.3),
     layers.Dense(1, activation = 'sigmoid')]
)

rmsprop = optimizers.RMSprop(learning_rate = 1e-4)
model.compile(
    optimizer = rmsprop, loss = 'binary_crossentropy', metrics = 'accuracy'
)

checkpoint_cb = tf.keras.callbacks.ModelCheckpoint('best-simplernn-model.h5', save_best_only = True)
earlystopping_cb = tf.keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)

history = model.fit(train_seq, train_target, epochs=100, batch_size=64, validation_data=(val_seq, val_target),
                    callbacks=[checkpoint_cb, earlystopping_cb])
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.xlabel('epoch')
plt.ylabel('loss')
plt.show()
